{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f07655f-99c5-4e2a-8dad-acf4b3d714f3",
   "metadata": {},
   "source": [
    "# Detect Emotions in Images\n",
    "\n",
    "One of the easiest, and yet also the most effective, ways of analyzing how people feel is looking at their facial expressions. Most of the time, our face best describes how we feel in a particular moment. This means that emotion recognition is a simple multiclass classification problem. We need to analyze a person's face and put it in a particular class, where each class represents a particular emotion. In Python, we can use the `DeepFace` and `FER` libraries to detect emotions in images. \n",
    "\n",
    "One of the easiest, and yet also the most effective, ways of analyzing how people feel is looking at their facial expressions. Most of the time, our face best describes how we feel in a particular moment. This means that emotion recognition is a simple multiclass classification problem. We need to analyze a person's face and put it in a particular class, where each class represents a particular emotion.\n",
    "\n",
    "Analyzing faces is not always enough to gauge how somebody feels. Humans often try to hide how they feel. This can sometimes lead to misleading results if only emotion recognition in images is performed. However, in combination with other techniques (such as body language in images, or voice analysis in videos), we can get a pretty solid idea of how somebody feels.\n",
    "\n",
    "Let's demonstrate how easy it is to perform emotion detection in images.  We can use pre-built libraries that will allow us to easily analyze faces and get the results we want very quickly without using too much code.\n",
    "\n",
    "## Links:\n",
    "- https://www.edlitera.com/blog/posts/emotional-artificial-intelligence-education\n",
    "- https://www.paulekman.com/blog/science-of-smiling/\n",
    "- https://www-users.cse.umn.edu/~qzhao/emotionalattention.html - a more general sentiment in images (not just facial expressions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c22dae8-77fc-4892-991e-e30e4fc4d97c",
   "metadata": {},
   "source": [
    "## Displaying images\n",
    "\n",
    "`class IPython.display.Image(data=None, url=None, filename=None, format=None, embed=None, ` <br>\n",
    "`   width=None, height=None, retina=False, unconfined=False, metadata=None, alt=None)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05d6aaf-acdf-4eb6-b26f-908f71b3ea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "listOfImageNames = ['data/images/economist01.jpg',\n",
    "                    'data/images/economist02.jpg',\n",
    "                   'data/images/economist03.jpg']\n",
    "\n",
    "for imageName in listOfImageNames:\n",
    "    display(Image(filename=imageName,width=300))\n",
    "    print(imageName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb821cdd-e7ab-48ed-a9e6-dbd06ba967d2",
   "metadata": {},
   "source": [
    "## The `DeepFace` Library\n",
    "\n",
    "The first library we are going to talk about is the `DeepFace` library. It is probably the most popular library for performing emotion analysis and similar tasks. `DeepFace` is an open-source project licensed under the MIT license. This means users can freely use, modify and distribute the library both for non-commercial and commercial purposes. This makes it perfect for anybody who might want to implement it in their practices. It serves as a framework for using already-trained deep learning models to perform image and video analysis. It offers much more than just emotion detection, even if that is what interests us most. \n",
    "\n",
    "The library uses pre-trained SOTA models (State of the Art models) in the background. SOTA models are those models that currently achieve the best possible results for some particular task on a set of benchmark datasets. The models `DeepFace` uses in the background are:\n",
    "\n",
    "- VGG-Face\n",
    "- Google FaceNet\n",
    "- OpenFace\n",
    "- Facebook DeepFace\n",
    "- DeepID\n",
    "- ArcFace\n",
    "- Dlib\n",
    "\n",
    "These models are so good that they have demonstrated that they can analyze images of faces (and even videos) at a level that surpasses what is humanly possible. The face recognition pipeline of `DeepFace` consists of four stages: \n",
    "\n",
    "- detection \n",
    "- alignment\n",
    "- representation \n",
    "- verification\n",
    "\n",
    "Let's demonstrate how `DeepFace` performs all of the aforementioned tasks using only one line of code.\n",
    "\n",
    "### Installation\n",
    "\n",
    "`%pip install deepface`\n",
    "\n",
    "This will automatically install everything we need to use this library. Using the library is extremely simple. After we import the package, we just need to input an image. The library will give us a detailed analysis of that image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb25a3e9-b31a-45ff-941a-a1e9949ad551",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install deepface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a22457-eb7c-4d3d-9b90-e758806d2248",
   "metadata": {},
   "source": [
    "### Using the `DeepFace` Library\n",
    "Let's demonstrate how DeepFace works on the following image:\n",
    "\n",
    "<figure>\n",
    "    <img src=\"data/images/economist03.jpg\" style=\"width:300px\">\n",
    "    <figcaption align = \"center\">\n",
    "        <b>Figure 1</b>: Source: The Economist\n",
    "    </figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf15d58-94bc-4558-8864-39c05f47b5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b765b78e-acdf-4957-81d3-f273cb03a0ba",
   "metadata": {},
   "source": [
    "Then we can analyze the face present in the image. And this is all there is to it if you don't want to customize the process of analysis too much. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76be46db-988b-4281-a498-9159bedc177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_analysis = DeepFace.analyze(img_path = \"data/images/economist03.jpg\")\n",
    "face_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b376351-6110-48a9-9c22-d86e48d9f3b2",
   "metadata": {},
   "source": [
    "As you can see, we are given a very detailed analysis. It gives us the following information:\n",
    "\n",
    "- percentages for each of the 7 basic human emotions, and which is the dominant one\n",
    "- the bounding box coordinates for the face in the image with the region parameter\n",
    "- the predicted age of the person\n",
    "- the predicted gender of the person\n",
    "- the predicted race of the person (with percentages for different races)\n",
    "\n",
    "Since the result we get is a dictionary, we can easily access different parts of it by referencing the keys of the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015ca6a9-cab0-460f-99f7-c980e0bfc9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(face_analysis[0][\"emotion\"])\n",
    "print(face_analysis[0][\"dominant_emotion\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d478788b-3db5-43c7-8d72-84844d40a239",
   "metadata": {},
   "source": [
    "Let's try another image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd68fa4-14e7-40ab-83a2-3772fd312775",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_analysis = DeepFace.analyze(img_path = \"data/images/economist01.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3456d5-7c30-401e-abed-83a6de71df9f",
   "metadata": {},
   "source": [
    "In case a face is hard to detect in an image, employ the option `enforce_detection=False` as in the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955a07e0-7c1d-4f68-8048-953fb20150f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_analysis = DeepFace.analyze(img_path = \"images/economist01.jpg\", enforce_detection=False)\n",
    "face_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305d225a-2af9-475d-ba8e-176c8f590d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_analysis = DeepFace.analyze(img_path = \"images/economist02.jpg\", enforce_detection=False)\n",
    "face_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b951a89-69a0-44a6-a834-a005ad9a3132",
   "metadata": {},
   "source": [
    "### Defining a function to automate the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc4cd3a-212c-40dc-bf06-4e53ca51d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AnalyseFace(data,enforce=True):\n",
    "    try:\n",
    "        face_analysis = DeepFace.analyze(img_path = data, enforce_detection=enforce)\n",
    "    except ValueError:\n",
    "        face_analysis = DeepFace.analyze(img_path = data, enforce_detection=False)\n",
    "    display(Image(filename=data,width=300))\n",
    "    \n",
    "    print(f\"Dominant emotion: \\t{face_analysis[0]['dominant_emotion']:<10}\")\n",
    "    print(f\"Dominant race: \\t\\t{face_analysis[0]['dominant_race']:<10}\")\n",
    "    \n",
    "    #print(f\"Gender: \\t\\t{face_analysis[0]['gender']:<10}\")\n",
    "    print(f\"Age: \\t\\t\\t{face_analysis[0]['age']:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefcd808-3007-4059-a746-5fe86f9c0f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A \"believable\" example\n",
    "AnalyseFace('images/economist01.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4da6090-bf83-4cac-8706-f858dbbe3e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A failed example\n",
    "AnalyseFace('images/economist02.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d60ca9-e3ab-41a2-ad1d-e8b2e02eb921",
   "metadata": {},
   "source": [
    "In case of inefficient results, another option is to use another library. One such library is the `FER` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ecc712-d8f9-4ab3-874b-b4f73341b65b",
   "metadata": {},
   "source": [
    "## The `FER` Library\n",
    "\n",
    "The **Facial Expression Recognition (FER)** library is an open-source library created and maintained by Justin Shenk, co-founder of VisioLab, that allows us to perform emotion recognition on both images and videos with just a few lines of code. It is **not as versatile** as the `DeepFace` library. We **can only use it for emotion recognition**. Nonetheless, it is still very powerful, and in our case practical since it works out-of-the-box, even with images of low quality.  \n",
    "\n",
    "The library combines deep learning with OpenCV functionalities to perform emotion recognition in images. The way it works is pretty simple. We pass an image to the FER constructor, which gets initialized using either the OpenCV Haar Cascade classifier or a multi cascade convolutional network (MTCNN). As a result, we get an array of values assigned to each of the aforementioned basic emotions, in percentages between 0 and 1. If we want, we can also access just the value of the dominant emotion.  Let's demonstrate how analyzing emotions using FER works.\n",
    "\n",
    "### Installation\n",
    "\n",
    "`!pip install fer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3754d8d8-8ade-40cb-bd6f-06cd34fadb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fer  # the usual \"%pip install\" did not work, only \"!pip install\" workd. Reason: unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df0aaff-2715-4925-8746-bebc2025ed01",
   "metadata": {},
   "source": [
    "Using the `FER` lirary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163b3451-d97e-42e6-a807-28fd1e5d7483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fer import FER\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6666e21e-0a3f-4540-bd95-bba0b0563bb0",
   "metadata": {},
   "source": [
    "Now we can define our emotion detector. For this example let's use **MTCNN**. If we set the argument `mtcnn=True`, the detector will use the **MTCNN**. If we set it to `False`, it will use the **Haar Cascade classifier**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680efd79-5a48-45c7-aaa6-202efb42328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_detector = FER(mtcnn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4344110d-8608-4218-bbd3-d906e6224f19",
   "metadata": {},
   "source": [
    "We can now define the image that we want to analyze. Let's use an image that has multiple faces in it to demonstrate that FER can analyze multiple faces at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077be127-707f-414c-bf0a-6473af3a34b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = cv2.imread(\"images/multifaces.webp\")\n",
    "display(Image(filename=\"images/multifaces.png\",width=300))\n",
    "analysis = emotion_detector.detect_emotions(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e953b8f6-98fa-48cd-82f3-b18836cb30d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b948e0c-8d8c-4649-b870-cdbee7642826",
   "metadata": {},
   "source": [
    "The result we get is a list of dictionaries, where each dictionary represents one face. It provides us with bounding box coordinates and an analysis of the emotions of the people in the images. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598e5500-cb3f-4cfd-9667-1bbe306e81f5",
   "metadata": {},
   "source": [
    "### Defining a function to automate the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0141958e-086c-4f6c-8494-e792dc86b049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AnalyseFaceEmotion(data,MTCNN=True):\n",
    "    emotion_detector = FER(mtcnn=MTCNN)\n",
    "    test_img = cv2.imread(data)\n",
    "    display(Image(filename=data,width=300))\n",
    "    analysis = emotion_detector.detect_emotions(test_img)\n",
    "  \n",
    "    print(analysis[0][\"emotions\"])\n",
    "    \n",
    "    dominant_emotion, emotion_score = emotion_detector.top_emotion(test_img)\n",
    "    print(f\"Dominant emotion: \\t{dominant_emotion, emotion_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223d5859-0dd0-4b2d-b8d2-cecd35e34eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "AnalyseFaceEmotion('images/economist01.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc056ad0-d162-4ebb-bbf4-f1824449a108",
   "metadata": {},
   "outputs": [],
   "source": [
    "AnalyseFaceEmotion('images/economist02.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239f34c1-61f5-47fa-9851-6ecbf2b2a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "AnalyseFaceEmotion('images/economist03.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0748b48-a181-4429-b75d-845a72fc2369",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Emotion recognition is a field that keeps advancing at tremendous speed. One of the most important aspects of analyzing emotions is analyzing human faces. While the technology is still not perfect, advanced emotion recognition models outperform humans when it comes to emotion recognition in images. Of course, there are certain limitations to models, such as the one we demonstrated when working with the DeepFace library. However, most of the time, the results we get are pretty reliable.\n",
    "\n",
    "While it is possible to build a custom model, for over 90% of users that is not necessary. The libraries that do exist are open-source, can be used for both commercial and non-commercial purposes, and allow users to perform emotion recognition using just a few lines of code.\n",
    "\n",
    "Probably the most popular libraries for performing emotion recognition are DeepFace and FER. In this article, we demonstrated how to use both of them, and we also pointed out the advantages and disadvantages of each of the two libraries. In tandem, they form the perfect duo for performing emotion recognition.\n",
    "\n",
    "In the next article of this series, we will demonstrate how to perform emotion recognition on videos. We will try to predict whether a student is interested in a particular lecture or not. This could in the future become a very powerful tool that helps teachers, professors, and those in the education industry better cater to the needs of their students and make education more effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edc3ff8-ac28-4595-b8fe-e08f9d8438f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
